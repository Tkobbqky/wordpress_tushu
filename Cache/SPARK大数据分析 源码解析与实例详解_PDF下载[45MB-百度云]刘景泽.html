SPARK大数据分析:源码解析与实例详解 PDF下载 刘景泽 百度云 电子书 下载 电子书下载
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#712137051
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#712137051
<p>书名:SPARK大数据分析:源码解析与实例详解</p><p>作者:刘景泽</p><p>页数:436</p><p>定价:¥89.0</p><p>出版社:电子工业出版社</p><p>出版日期:2019-07-01</p><p>ISBN:9787121370519</p><p><h2>本书特色</h2></p>[<p>
讲解了网络大数据时代应运而生的、能高效迅捷地分析处理数据的工具——Spark，它带领读者快速掌握用 Spark 收集、计算、简化和保存海量数据的方法，学会交互、迭代和增量式分析，解决分区、数据本地化和自定义序列化等问题。
                                        </p>]<p><h2>内容简介</h2></p>[<p>讲解了网络大数据时代应运而生的、能高效迅捷地分析处理数据的工具――Spark，它带领读者快速掌握用 Spark 收集、计算、简化和保存海量数据的方法，学会交互、迭代和增量式分析，解决分区、数据本地化和自定义序列化等问题。</p>]<p><h2>作者简介</h2></p>[<p>全栈工程师，长期涉足大数据的研发工作。拥有丰富的大数据经验，曾担任多家知名企的主力研发，同时负责大数据云服务组件开发。精通Java、Scala、Python等多种编程语言，擅长大数据生态圈的研发、算法、逆向工程等技术。为多家企业提供技术支持，并长期面向企业一线开发人员分享实战经验。</p>]<p><h2>目录</h2></p>
    =第1篇  准备
-
第1章  认识大数据和Spark	2
1.1  大数据的介绍	2
1.2  Apache Spark能做什么	3
1.3  其他分布式数据处理框架	4
1.4  如何使用本书	4
?O1.4.1  需要提前具备的基础	4
?O1.4.2  准备相关开发环境	4
?O1.4.3  如何学习本书	5
-
第2章  安装与配置Spark集群	6
2.1  下载Spark安装包	6
2.2  检查与准备集群环境	7
2.3  了解目前集群中已经部署的框架服务	11
2.4  部署Spark集群	12
?O2.4.1  实例1：基于Standalone模式部署Spark集群	12
?O2.4.2  实例2：部署Spark的历史服务——Spark History Server	16
?O2.4.3  实例3：基于Standalone模式部署高可用的Master服务	18
?O2.4.4  实例4：基于YARN模式部署Spark集群	20
?O2.4.5  Standalone模式与YARN模式的特点	22
2.5  本章小结	23
-
第3章  第1个Spark程序	24
3.1  运行第1个Spark程序	24
?O3.1.1  实例5：基于Standalone模式运行第1个Spark程序	24
?O3.1.2  实例6：基于YARN模式运行第1个Spark程序	27
?O3.1.3  提交Spark程序时的参数规范	30
3.2  使用spark-shell编写并运行WordCount程序	30
?O3.2.1  实例7：启动spark-shell	31
?O3.2.2  实例8：在spark-shell中编写WordCount程序	32
3.3  使用IDEA编写并运行WordCount程序	34
?O3.3.1  实例9：准备开发环境，并构建代码工程	34
?O3.3.2  实例10：使用IDEA编写WordCount程序	41
?O3.3.3  实例11：在IDEA中本地运行WordCount程序	44
?O3.3.4  实例12：在IDEA中远程运行WordCount程序	46
?O3.3.5  实例13：打包程序，并提交至集群运行	48
3.4  本章小结	49
=
第2篇  入门
-
第4章  读写分布式数据——基于Spark Core	52
4.1  RDD的诞生	52
4.2  进一步理解RDD	53
?O4.2.1  数据存储	53
?O4.2.2  数据分析	55
?O4.2.3  程序调度	56
4.3  读取数据并生成RDD	57
?O4.3.1  实例14：读取普通文本数据	58
?O4.3.2  实例15：读取JSON格式的数据	59
?O4.3.3  实例16：读取CSV、TSV格式的数据	61
?O4.3.4  实例17：读取SequenceFile格式的数据	62
?O4.3.5  实例18：读取Object格式的数据	64
?O4.3.6  实例19：读取HDFS中的数据——显式调用Hadoop API	66
?O4.3.7  实例20：读取MySQL数据库中的数据	68
4.4  保存RDD中的数据到外部存储	70
?O4.4.1  实例21：保存成普通文本文件	70
?O4.4.2  实例22：保存成JSON文件	71
?O4.4.3  实例23：保存成CSV、TSV文件	73
?O4.4.4  实例24：保存成SequenceFile文件	74
?O4.4.5  实例25：保存成Object文件	75
?O4.4.6  实例26：保存成HDFS文件——显式调用Hadoop API的方式	76
?O4.4.7  实例27：写入MySQL数据库	78
4.5  本章小结	80
-
第5章  处理分布式数据——基于Spark Core	81
5.1  RDD的转换（transformations）操作——转换数据形态	81
?O5.1.1  实例28：基础转换操作	81
?O5.1.2  实例29：键值对转换操作	103
5.2  RDD的行动（actions）操作——触发执行任务计划	115
?O5.2.1  实例30：基础行动操作	116
?O5.2.2  实例31：键值对行动操作	125
?O5.2.3  实例32：数值行动操作	127
5.3  本章小结	128
=
第3篇  进阶
-
第6章  RDD的高级操作	130
6.1  缓存RDD	130
?O6.1.1  缓存RDD的基础知识	130
?O6.1.2  实例33：缓存与释放RDD	133
6.2  RDD的检查点（Checkpoint）机制	139
?O6.2.1  了解Checkpoint机制	139
?O6.2.2  实例34：使用Checkpoint机制	141
?O6.2.3  Checkpoint机制的工作流程	144
6.3  RDD的依赖关系	145
?O6.3.1  窄依赖（narrow dependencies）	145
?O6.3.2  宽依赖（wide/shuffle dependencies）	148
?O6.3.3  实例35：让子RDD混合依赖依赖多个父RDD	151
?O6.3.4  实例36：词频统计——总结运算过程涉及的概念	153
6.4  累加器（Accumulator）	155
?O6.4.1  认识累加器	155
?O6.4.2  实例37：使用系统累加器1——长整数、双精度浮点数累加器	156
?O6.4.3  实例38：使用系统累加器2——集合累加器	159
?O6.4.4  实例39：自定义累加器	160
6.5  广播（Broadcast）——将数据块缓存到所有节点	164
?O6.5.1  认识广播	165
?O6.5.2  实例40：使用广播补全数据	165
6.6  本章小结	168
-
第7章  用SQL语法分析结构化数据——基于Spark SQL	169
7.1  为什么会产生Spark SQL	169
7.2  认识DataFrame与Dataset数据类型	170
?O7.2.1  认识DataFrame	170
?O7.2.2  认识Dataset	171
7.3  实例41：通过Dataset、DataFrame分析用户数据	172
?O7.3.1  用spark-shell编写程序	172
?O7.3.2  用IDEA编写程序	175
7.4  不同Spark版本的操作差异	177
?O7.4.1  认识SQLContext与HiveContext	178
?O7.4.2  认识SparkSession	178
7.5  DataFrame、Dataset的基本操作	179
?O7.5.1  DSL与SQL的语法风格	179
?O7.5.2  使用临时视图的注意事项	181
?O7.5.3  实例42：读取JSON、CSV格式的数据	183
?O7.5.4  实例43：读取Parquet格式的数据	185
?O7.5.5  实例44：读取代码中动态生成的数据	185
?O7.5.6  实例45：读取关系型数据库中的数据	188
?O7.5.7  实例46：输出Dataset、DataFrame中的数据	189
?O7.5.8  实例47：RDD、DataFrame、Dataset之间的相互转换	192
7.6  用户自定义函数	195
?O7.6.1  实例48：实现“一进一出”的UDF	195
?O7.6.2  实例49：实现“多进一出”的UDAF	198
?O7.6.3  实例50：实现“一进多出”的UDTF	208
7.7  集成Spark SQL与Hive	211
?O7.7.1  已经部署Hive框架	211
?O7.7.2  尚未部署Hive框架	215
7.8  本章小结	215
-
第8章  实时处理流式数据——基于Spark Streaming	216
8.1  为什么会产生Spark Streaming	216
8.2  第1个Spark Streaming程序	216
?O8.2.1  实例51：用spark-shell编写程序	216
?O8.2.2  实例52：用IDEA编写程序	221
8.3  什么是DStream	222
?O8.3.1  认识DStream	222
?O8.3.2  认识DStreamGraph	223
8.4  读取数据到DStream中	227
?O8.4.1  实例53：读取HDFS文件夹中的数据	227
?O8.4.2  实例54：读取RDD组成的数据队列	229
?O8.4.3  实例55：实时读取Flume中的数据	230
?O8.4.4  实例56：用高阶API实时读取Kafka中的数据	235
?O8.4.5  实例57：用低阶API实时读取Kafka中的数据	242
8.5  Spark Streaming中的几个时间概念	251
?O8.5.1  批处理间隔	251
?O8.5.2  窗口时间宽度与滑动时间宽度	252
?O8.5.3  实例58：使用窗口操作，每两秒钟统计10秒内的平均温度	254
8.6  DStream的操作总结	259
?O8.6.1  DStream的操作说明	259
?O8.6.2  实例59：直接面向DStream中的RDD进行数据分析	261
?O8.6.3  实例60：将DStream中的数据实时输出至外部存储系统	263
?O8.6.4  实例61：对Dstream进行join操作	267
8.7  DStream中的转换分类	269
?O8.7.1  无状态转换	269
?O8.7.2  有状态转换	270
?O8.7.3  实例：用有状态转换做全局词频统计	270
8.8  在Spark Streaming中的缓存与Checkpoint	272
?O8.8.1  认识Spark Streaming中的Checkpoint	273
?O8.8.2  实例62：使用Spark Streaming中的Checkpoint	273
8.9  Spark Streaming中的累加器与广播变量	276
?O8.9.1  认识累加器与广播变量	276
?O8.9.2  实例63：自定义累加器，并结合无状态转换，实现实时的全局词频统计	276
8.10  关闭Spark Streaming程序	280
?O8.10.1  关闭程序的方案	281
?O8.10.2  实例64：合理关闭一个运行中的Spark Streaming程序	281
8.11  本章小结	284
=
第4篇  高阶
-
第9章  实时处理流式数据——基于Structured Streaming	286
9.1  为什么会产生Structured Streaming	286
9.2  第1个Structured Streaming程序	287
?O9.2.1  实例65：用spark-shell编写程序	287
?O9.2.2  实例66：用IDEA编写程序	289
9.3  Structured Streaming的编程模型	291
9.4  输入数据——生成Streaming Dataset、 Streaming DataFrame	292
?O9.4.1  实例67：根据文件生成工作流	292
?O9.4.2  实例68：根据文件、文件夹生成自动分区的工作流	295
?O9.4.3  实例69：根据Kafka以Streaming模式生成工作流	297
?O9.4.4  实例70：以Kafka为数据源，通过Batch方式生成工作流	300
?O9.4.5  实例71：根据指定速率生成工作流	304
9.5  基于事件时间的窗口操作	305
?O9.5.1  事件时间窗口的工作方式	305
?O9.5.2  实例72：事件时间窗口的生成规则	307
?O9.5.3  实例73：基于事件时间窗口实现词频统计	311
9.6  基于Watermark处理延迟数据	314
?O9.6.1  Watermark的作用	314
?O9.6.2  实例74：基于Update模式实现词频统计，并结合Watermark处理延迟数据	314
?O9.6.3  实例75：基于Append模式实现词频统计，并结合Watermark处理延迟数据	320
?O9.6.4  Watermark的底层工作原理	322
?O9.6.5  总结：Watermark机制与输出模式	329
9.7  实例76：在处理流式数据时去除重复数据	330
9.8  Structured Streaming中的join操作	332
?O9.8.1  实例77：在Stream-Static模式下的inner join操作	333
?O9.8.2  实例78：在Stream-Stream模式下的inner join操作	335
?O9.8.3  总结：已经支持的join操作	340
9.9  在Structured Streaming中实现数据分组， 并手动维护分组状态	3
