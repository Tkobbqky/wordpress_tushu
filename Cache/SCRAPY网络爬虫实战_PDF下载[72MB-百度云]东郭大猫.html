SCRAPY网络爬虫实战 PDF下载 东郭大猫 百度云 电子书 下载 电子书下载
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#730253620
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#730253620
<p>书名:SCRAPY网络爬虫实战</p><p>作者:东郭大猫</p><p>页数:0</p><p>定价:¥59.0</p><p>出版社:清华大学出版社</p><p>出版日期:2019-10-01</p><p>ISBN:9787302536208</p><p><h2>本书特色</h2></p>[<p>
随着大数据分析、大数据计算火热兴起，越来越多的企业发布了数据分析岗位，而数据分析的基础则是海量的数据。Python中的Scrapy框架就是为了抓取数据而设计的。本书是一本Scrapy爬虫框架零基础起步的实战图书。 本书共分11章，第1~2章介绍Python环境的搭建、编辑器的使用、爬虫的一些基础知识（urllib、requests、Selenium、Xpath、CSS、正则表达式、BeautifulSoup库）等。第3~8章主要介绍Scrapy框架的原理与使用。第9~11章主要介绍Scrapy的优化，包括内置服务、组件优化等，*后通过一个完整的大型示例对全书的知识点做了应用总结。 本书入门门槛低、浅显易懂，适合所有Scrapy爬虫和数据分析行业的入门读者学习，也适合高等院校和培训学校作为爬虫入门教材和训练手册。
                                        </p>]<p><h2>内容简介</h2></p>[<p>随着大数据分析、大数据计算火热兴起，越来越多的企业发布了数据分析岗位，而数据分析的基础则是海量的数据。Python中的Scrapy框架就是为了抓取数据而设计的。本书是一本Scrapy爬虫框架零基础起步的实战图书。
本书共分11章，~2章介绍Python环境的搭建、编辑器的使用、爬虫的一些基础知识（urllib、requests、Selenium、Xpath、CSS、正则表达式、BeautifulSoup库）等。第3~8章主要介绍Scrapy框架的原理与使用。第9~11章主要介绍Scrapy的优化，包括内置服务、组件优化等，很后通过一个完整的大型示例对全书的知识点做了应用总结。
本书入门门槛低、浅显易懂，适合所有Scrapy爬虫和数据分析行业的入门读者学习，也适合高等院校和培训学校作为爬虫入门教材和训练手册。</p>]<p><h2>作者简介</h2></p>[<p>东郭大猫，常年从事数据相关的工作，擅长用Python进行数据的爬取、分析和存储。使用Python超过5年，做过公司内部的数据爬取培训。</p>]<p><h2>目录</h2></p>
    目    录第1章  Python开发环境的搭建 11.1  PYTHON SDK安装 11.1.1  在Windows上安装Python 11.1.2  在Ubuntu上安装Python 21.2  安装开发工具PYCHARM社区版 31.3  安装开发工具VISUAL STUDIO社区版 5第2章  爬虫基础知识 62.1  爬虫原理 62.1.1  爬虫运行基本流程 62.1.2  HTTP请求过程 82.2  网页分析方法1：浏览器开发人员工具 92.2.1  Elements面板 102.2.2  Network面板 112.3  网页分析方法2：XPATH语法 142.3.1  XPath节点 142.3.2  XPath语法 152.3.3  XPath轴 172.3.4  XPath运算符 192.4  网页分析方法3：CSS选择语法 192.4.1  元素选择器 202.4.2  类选择器 212.4.3  ID选择器 212.4.4  属性选择器 212.4.5  后代选择器 212.4.6  子元素选择器 222.4.7  相邻兄弟选择器 222.5  网页分析方法4：正则表达式 222.5.1  提取指定字符 232.5.2  预定义字符集 232.5.3  数量限定 232.5.4  分支匹配 242.5.5  分组 242.5.6  零宽断言 242.5.7  贪婪模式与非贪婪模式 252.5.8  Python中的正则表达式 252.6  爬虫常用类库1：PYTHON中的HTTP基本库URLLIB 302.6.1  发送请求 302.6.2  使用Cookie 312.7  爬虫常用类库2：更人性化的第三方库REQUESTS 332.7.1  发送请求 342.7.2  请求头 352.7.3  响应内容 352.7.4  响应状态码 362.7.5  cookies参数 372.7.6  重定向与请求历史 372.7.7  超时 382.7.8  设置代理 382.7.9  会话对象 382.8  爬虫常用类库3：元素提取利器BEAUTIFULSOUP 392.8.1  安装BeautifulSoup 392.8.2  安装解析器 402.8.3  BeautifulSoup使用方法 412.8.4  BeautifulSoup对象 432.8.5  遍历文档树 472.8.6  搜索文档树 522.8.7  BeautifulSoup中的CSS选择器 572.9  爬虫常用类库4：SELENIUM操纵浏览器 582.9.1  安装Selenium 592.9.2  Selenium的基本使用方法 592.9.3  Selenium Webdriver的原理 612.9.4  Selenium中的元素定位方法 612.9.5  Selenium Webdriver基本操作 632.9.6  Selenium实战：抓取拉钩网招聘信息 642.10  爬虫常用类库5：SCRAPY爬虫框架 672.10.1  安装Scrapy 672.10.2  Scrapy简介 682.11  基本爬虫实战：抓取CNBETA网站科技类文章 692.11.1  URL管理器 702.11.2  数据下载器 712.11.3 数据分析器 722.11.4  数据保存器 742.11.5  调度器 75第3章  Scrapy命令行与Shell 783.1  SCRAPY命令行介绍 783.1.1  使用startproject创建项目 803.1.2  使用genspider创建爬虫 813.1.3  使用crawl启动爬虫 823.1.4  使用list查看爬虫 823.1.5  使用fetch获取数据 833.1.6  使用runspider运行爬虫 843.1.7  通过view使用浏览器打开URL 853.1.8  使用parse测试爬虫 853.2  SCRAPY SHELL命令行 853.2.1  Scrapy Shell的用法 853.2.2  实战：解析名人名言网站 86第4章  Scrapy爬虫 894.1  编写爬虫 894.1.1  scrapy.Spider爬虫基本类 894.1.2  start_requests()方法 904.1.3  parse(response)方法 914.1.4  Selector选择器 914.2  通用爬虫 944.2.1  CrawlSpider 944.2.2  XMLFeedSpider 954.2.3  CSVFeedSpider 964.2.4  SitemapSpider 974.3  爬虫实战 984.3.1  实战1：CrawlSpider爬取名人名言 984.3.2  实战2：XMLFeedSpider爬取伯乐在线的RSS 1024.3.3  实战3：CSVFeedSpider提取csv文件数据 1044.3.4  实战4：SitemapSpider爬取博客园文章 106第5章  Scrapy管道 1095.1  管道简介 1095.2  编写自定义管道 1105.3  下载文件和图片 1135.3.1  文件管道 1145.3.2  图片管道 1175.4  数据库存储MYSQL 1215.4.1  在Ubuntu上安装MySQL 1215.4.2  在Windows上安装MySQL 1225.4.3  MySQL基础 1255.4.4  MySQL基本操作 1275.4.5  Python操作MySQL 1295.5  数据库存储MONGODB 1315.5.1  在Ubuntu上安装MongoDB 1325.5.2  在Windows上安装MongoDB 1325.5.3  MongoDB基础 1355.5.4  MongoDB基本操作 1375.5.5  Python操作MongoDB 1435.6  实战：爬取链家二手房信息并保存到数据库 144第6章  Request与Response 1576.1  REQUEST对象 1576.1.1  Request类详解 1586.1.2  Request回调函数与错误处理 1606.2  RESPONSE 1626.2.1  Response类详解 1626.2.2  Response子类 163第7章  Scrapy中间件 1657.1  编写自定义SPIDER中间件 1657.1.1  激活中间件 1657.1.2  编写Spider中间件 1667.2  SPIDER内置中间件 1687.2.1  DepthMiddleware爬取深度中间件 1687.2.2  HttpErrorMiddleware失败请求处理中间件 1687.2.3  OffsiteMiddleware过滤请求中间件 1697.2.4  RefererMiddleware参考位置中间件 1697.2.5  UrlLengthMiddleware网址长度限制中间件 1707.3  编写自定义下载器中间件 1707.3.1  激活中间件 1707.3.2  编写下载器中间件 1717.4  下载器内置中间件 1737.4.1  CookiesMiddleware 1737.4.2  HttpProxyMiddleware 1747.5  实战：为爬虫添加中间件 174第8章  Scrapy配置与内置服务 1788.1  SCRAPY配置简介 1788.1.1  命令行选项（优先级*高） 1788.1.2  每个爬虫内配置 1798.1.3  项目设置模块 1798.1.4  默认的命令行配置 1818.1.5  默认全局配置（优先级*低） 1828.2  日志 1828.3  数据收集 1848.4  发送邮件 1878.4.1  简单例子 1878.4.2  MailSender类 1878.4.3  在settings.py中对Mail进行设置 1888.5  实战：抓取猫眼电影TOP100榜单数据 1888.5.1  分析页面元素 1898.5.2  创建项目 1898.5.3  编写items.py 1908.5.4  编写管道pipelines.py 1908.5.5  编写爬虫文件top100.py 191第9章  模拟登录 1949.1  模拟提交表单 1949.2  用COOKIE模拟登录状态 1979.3  项目实战 1989.3.1  实战1：使用FormRequest模拟登录豆瓣 1989.3.2  实战2：使用Cookie登录 202第10章  Scrapy爬虫优化 20510.1  SCRAPY MONGODB实战：抓取并保存IT之家博客新闻 20510.1.1  确定目标 20510.1.2  创建项目 20610.1.3  编写items.py文件 20710.1.4  编写爬虫文件news.py 20710.1.5  编写管道pipelines.py 20910.1.6  编写settings.py 21010.1.7  运行爬虫 21110.2  用BENCHMARK进行本地环境评估 21210.3  扩展爬虫 21410.3.1  增大并发 21410.3.2  关闭Cookie 21410.3.3  关闭重试 21410.3.4  减少下载超时时间 21510.3.5  关闭重定向 21510.3.6  AutoThrottle扩展 215第11章  Scrapy项目实战：爬取某社区用户详情 21711.1  项目分析 21711.1.1  页面分析 21711.1.2  抓取流程 22111.2  创建爬虫 22111.2.1  cookies收集器 22211.2.2  Items类 22511.2.3  Pipeline管道编写 22611.2.4  Spider爬虫文件 22711.2.5  Middlewars中间件编写 235
 
