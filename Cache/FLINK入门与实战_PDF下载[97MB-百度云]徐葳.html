FLINK入门与实战 PDF下载 徐葳 百度云 电子书 下载 电子书下载
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#711551678
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#711551678
<p>书名:FLINK入门与实战</p><p>作者:徐葳</p><p>页数:226</p><p>定价:¥59.0</p><p>出版社:人民邮电出版社</p><p>出版日期:2018-02-01</p><p>ISBN:9787115516787</p><p><h2>本书特色</h2></p>[<p>
本书旨在帮助读者从零开始快速掌握Flink的基本原理与核心功能。本书首先介绍了Flink的基本原理和安装部署，并对Flink中的一些核心API进行了详细分析。然后配套对应的案例分析，分别使用Java代码和Scala代码实现案例。*后通过两个项目演示了Flink在实际工作中的一些应用场景，帮助读者快速掌握Flink开发。 学习本书需要大家具备一些大数据的基础知识，比如Hadoop、Kafka、Redis、Elasticsearch等框架的基本安装和使用。本书也适合对大数据实时计算感兴趣的读者阅读。 学习本书需要大家具备一些大数据的基础知识，例如Hadoop、Kafka、Redis、Elasticsearch等框架的基本安装和使用。本书也适合对大数据实时计算感兴趣的爱好者阅读。
                                        </p>]<p><h2>内容简介</h2></p>[<p>本书旨在帮助读者从零开始快速掌握Flink的基本原理与核心功能。本书首先介绍了Flink的基本原理和安装部署，并对Flink中的一些核心API进行了详细分析。然后配套对应的案例分析，分别使用Java代码和Scala代码实现案例。很后通过两个项目演示了Flink在实际工作中的一些应用场景，帮助读者快速掌握Flink开发。
学习本书需要大家具备一些大数据的基础知识，比如Hadoop、Kafka、Redis、Elasticsearch等框架的基本安装和使用。本书也适合对大数据实时计算感兴趣的读者阅读。

学习本书需要大家具备一些大数据的基础知识，例如Hadoop、Kafka、Redis、Elasticsearch等框架的基本安装和使用。本书也适合对大数据实时计算感兴趣的爱好者阅读。</p>]<p><h2>作者简介</h2></p>[<p>徐葳，拥有多年一线互联网公司软件的研发经验，曾担任猎豹移动大数据技术专家、中科院大数据研究院大数据技术专家、某大学外聘大数据讲师。他主导开发海外舆情监控系统、海量数据采集平台、OLAP数据分析平台、三度关系推荐系统和PB级数据检索系统等，并进行大数据相关的内容培训。此外，他对Hadoop、Storm和Spark等大数据技术框架有深入的理解。</p>]<p><h2>目录</h2></p>
    第 1章　Flink概述　　11.1　Flink原理分析　11.2　Flink架构分析　21.3　Flink基本组件　31.4　Flink流处理（Streaming）与批处理（Batch）　41.5　Flink典型应用场景分析　51.6　流式计算框架对比　61.7　工作中如何选择实时计算框架　8第　2章 Flink快速入门　92.1　Flink开发环境分析　92.1.1　开发工具推荐　92.1.2　Flink程序依赖配置　102.2　Flink程序开发步骤　112.3　Flink流处理（Streaming）案例开发　112.3.1　Java代码开发　122.3.2　Scala代码开发　142.3.3　执行程序　162.4　Flink批处理（Batch）案例开发　162.4.1　Java代码开发　162.4.2　Scala代码开发　182.4.3　执行程序　19第3章　Flink的安装和部署　203.1　Flink本地模式　203.2　Flink集群模式　223.2.1　Standalone模式　233.2.2　Flink on Yarn模式　263.2.3　yarn-session.sh命令分析　303.2.4　Flink run命令分析　303.3　Flink代码生成JAR包　313.4　Flink HA的介绍和使用　353.4.1　Flink HA　353.4.2　Flink Standalone集群的HA安装和配置　353.4.3　Flink on Yarn集群HA的安装和配置　503.5　Flink Scala Shell　53第4章　Flink常用API详解　564.1　Flink API的抽象级别分析　564.2　Flink DataStream的常用API　574.2.1　DataSource　574.2.2　Transformation　664.2.3　Sink　704.3　Flink DataSet的常用API分析　804.3.1　DataSource　804.3.2　Transformation　814.3.3　Sink　824.4　Flink Table API和SQL的分析及使用　824.4.1　Table API和SQL的基本使用　834.4.2　DataStream、DataSet和Table之间的转换　874.4.3　Table API和SQL的案例　914.5　Flink支持的DataType分析　974.6　Flink序列化分析　97第5章　Flink高级功能的使用　995.1　Flink Broadcast　995.2　Flink Accumulator　1045.3　Flink Broadcast和Accumulator的区别　1085.4　Flink Distributed Cache　108第6章　Flink State管理与恢复　1126.1　State　1126.1.1　Keyed State　1136.1.2　Operator State　1156.2　State的容错　1166.3　CheckPoint　1186.4　StateBackend　1196.5　Restart Strategy　1216.6　SavePoint　123第7章　Flink窗口详解　1257.1　Window　1257.2　Window的使用　1267.2.1　Time Window　1277.2.2　Count Window　1287.2.3　自定义Window　1297.3　Window聚合分类　1307.3.1　增量聚合　1307.3.2　全量聚合　132第8章　Flink Time详解　1348.1　Time　1348.2　Flink如何处理乱序数据　1358.2.1　Watermark　1368.2.2　Watermark的生成方式　1378.3　EventTime Watermark解决乱序数据的案例详解　1388.3.1　实现Watermark的相关代码　1388.3.2　通过数据跟踪Watermark的时间　1428.3.3　利用Watermark Window处理乱序数据　1498.3.4　Late Element的处理方式　1538.3.5　在多并行度下的Watermark应用　1638.3.6　With Periodic Watermarks案例总结　165第9章　Flink并行度详解　1669.1　Flink并行度　1669.2　TaskManager和Slot　1669.3　并行度的设置　1679.3.1　并行度设置之Operator Level　1689.3.2　并行度设置之Execution Environment Level　1689.3.3　并行度设置之Client Level　1699.3.4　并行度设置之System Level　1699.4　并行度案例分析　169第　10章 Flink Kafka Connector详解　17210.1　Kafka Connector　17210.2　Kafka Consumer　17310.2.1　Kafka Consumer消费策略设置　17310.2.2　Kafka Consumer的容错　17510.2.3　动态加载Topic　17610.2.4　Kafka Consumer Offset自动提交　17710.3　Kafka Producer　17710.3.1　Kafka Producer的使用　17710.3.2　Kafka Producer的容错　179第　11章 Flink实战项目开发　18411.1　实时数据清洗（实时ETL）　18411.1.1　需求分析　18411.1.2　项目架构设计　18411.1.3　项目代码实现　18611.2　实时数据报表　20511.2.1　需求分析　20511.2.2　项目架构设计　20611.2.3　项目代码实现　207
